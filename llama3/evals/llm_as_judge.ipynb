{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import re\n",
    "key = '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'\n",
    "client = OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama3.config import DATA_DIR\n",
    "# import os\n",
    "# kar_kcc = pd.read_csv(os.path.join(DATA_DIR, \"clean_karna_kcc.csv\"))\n",
    "# kar_kcc.columns = kar_kcc.columns.str.strip() \n",
    "# kar_kcc = kar_kcc.iloc[:100]\n",
    "# ft_ans = pd.read_csv(\"./agrillama_zeroshot_ans.csv\")\n",
    "# ft_inc_ans = pd.read_csv(\"./agrillama_fewshot_ans.csv\")\n",
    "# base_ans = pd.read_csv(\"./llama_zeroshot_ans.csv\")\n",
    "# base_inc_ans = pd.read_csv(\"./llama_fewshot_ans.csv\")\n",
    "# df = pd.DataFrame({'Dist': kar_kcc['DistrictName'],'Query': kar_kcc['QueryText'], 'KccAns': kar_kcc['KccAns'] ,'ft': ft_ans['Answer'], 'ft_inc': ft_inc_ans['Answer'], 'base': base_ans['Answer'], 'base_inc': base_inc_ans['Answer'] })\n",
    "# df.to_csv(\"ans_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df = pd.read_csv(\"./ans_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = \"\"\"Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "            AI assistants to the user question displayed below. Your evaluation should consider\n",
    "            correctness and helpfulness. You will be given a reference answer, assistant A’s answer,\n",
    "            and assistant B’s answer. Your job is to evaluate which assistant’s answer is better.\n",
    "            Begin your evaluation by comparing both assistants’ answers with the reference answer.\n",
    "            Identify and correct any mistakes. Avoid any position biases and ensure that the order in\n",
    "            which the responses were presented does not influence your decision. Do not allow the\n",
    "            length of the responses to influence your evaluation. Do not favor certain names of the\n",
    "            assistants. Be as objective as possible. After providing your explanation, output your\n",
    "            final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "            if assistant B is better, and \"[[C]]\" for a tie.       \n",
    "            \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to check if a row is clean\n",
    "# def judge(question, answer_ref, answer_a, answer_b):\n",
    "#     response = client.chat.completions.create(\n",
    "#     model=\"gpt-4\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": sys_msg\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": f\"\"\"[Query]\n",
    "#             {question}\n",
    "#             [The Start of Reference Answer]\n",
    "#             {answer_ref}\n",
    "#             [The End of Reference Answer]\n",
    "#             [The Start of Assistant A’s Answer]\n",
    "#             {answer_a}\n",
    "#             [The End of Assistant A’s Answer]\n",
    "#             [The Start of Assistant B’s Answer]\n",
    "#             {answer_b}\n",
    "#             [The End of Assistant B’s Answer]\n",
    "#             \"\"\"\n",
    "#         }\n",
    "       \n",
    "#     ],\n",
    "#     temperature=1,\n",
    "#     max_tokens=1088,\n",
    "#     top_p=1,\n",
    "#     frequency_penalty=0,\n",
    "#     presence_penalty=0,\n",
    "#     )\n",
    "#     output = response.choices[0].message.content.strip()\n",
    "#     # print(output)\n",
    "#     match = re.search(r'\\[\\[(.*?)\\]\\]', output)\n",
    "#     return output, match.group(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "}\n",
    "\n",
    "def judge_gemini(question=\"\", answer_ref=\"\", answer_a=\"\", answer_b=\"\"):\n",
    "  vertexai.init(project=\"agri-llms-421815\", location=\"us-central1\")\n",
    "  model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-preview-0514\",\n",
    "    system_instruction=[sys_msg]\n",
    "  )\n",
    "  responses = model.generate_content(\n",
    "      [f\"\"\"[Query]\n",
    "            {question}\n",
    "            [The Start of Reference Answer]\n",
    "            {answer_ref}\n",
    "            [The End of Reference Answer]\n",
    "            [The Start of Assistant A’s Answer]\n",
    "            {answer_a}\n",
    "            [The End of Assistant A’s Answer]\n",
    "            [The Start of Assistant B’s Answer]\n",
    "            {answer_b}\n",
    "            [The End of Assistant B’s Answer]\n",
    "            \"\"\"],\n",
    "      generation_config=generation_config,\n",
    "      safety_settings=safety_settings,\n",
    "      stream=False,\n",
    "  )\n",
    "  output = responses.candidates[0].content.parts[0]._raw_part.text\n",
    "  # for response in responses:\n",
    "  #   output = output + response.text\n",
    "  # output = responses.candidates[0].text\n",
    "  match = re.search(r'\\[\\[(.*?)\\]\\]', output)\n",
    "  return output, match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"ft_inc\"\n",
    "B = \"base_inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [08:22<00:00, 12.89s/it]\n"
     ]
    }
   ],
   "source": [
    "verdict = []\n",
    "eval = []\n",
    "for _, row in tqdm(ans_df.iloc[61:].iterrows(), total=len(ans_df.iloc[61:])):\n",
    "    time.sleep(10)\n",
    "    e, v = judge_gemini(f\"A farmer from {row['Dist']} district of Karnataka asked about {row['Query']}.\", row['KccAns'], row[A], row[B])\n",
    "    verdict.append(v)\n",
    "    eval.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = f\"{A}_vs_{B}\"\n",
    "pd.DataFrame({f'{dir}': eval}).to_csv(f\"{dir}_gemini_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({f'{dir}': verdict}).to_csv(f\"{dir}_gemini_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 12, 'A': 14, 'B': 13}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "\n",
    "for item in verdict:\n",
    "    if item in counts:\n",
    "        counts[item] += 1\n",
    "    else:\n",
    "        counts[item] = 1\n",
    "\n",
    "print(counts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 4\n",
    "{'agrillama few shot': 36, 'C': 47, 'llama few shot': 17}\n",
    "{'C': 45, 'agrillama zero shot': 33, 'llama zero shot': 22}\n",
    "\n",
    "## GEMINI Flash\n",
    "{'agrillama few shot': 43, 'C': 27, 'llama few shot': 30}\n",
    "{'C': 11, 'agrillama zero shot': 47, 'llama zero shot': 42}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
